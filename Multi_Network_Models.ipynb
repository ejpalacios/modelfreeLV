{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies models to all feeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45219,
     "status": "ok",
     "timestamp": 1706724096637,
     "user": {
      "displayName": "Anthony Gerard O'Malley",
      "userId": "08354629277054091219"
     },
     "user_tz": 0
    },
    "id": "uw4ybH0k3CWt",
    "outputId": "5d215add-1f9a-46fe-eae7-591f156719da"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn import set_config\n",
    "set_config(transform_output = \"pandas\")\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "rmse_scorer = make_scorer(lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared = False), greater_is_better=False)\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    HuberRegressor,\n",
    "    LinearRegression,\n",
    "    RANSACRegressor,\n",
    "    TheilSenRegressor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd()\n",
    "print(f\"{base_path=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feed_names = ['1 1', '1 2', '1 3', '1 4', '2 1', '2 2', '2 3', '2 4', '2 5', '3 1', '3 2', '3 3', '3 4', '3 5', '3 6', '4 1', '4 2', '4 3', '4 4', '4 5', '4 6', '5 1', '5 2', '5 3', '5 4', '5 5', '5 6', '5 7', '5 8', '6 1', '6 2', '7 1', '7 2', '7 3', '7 4', '7 5', '7 6', '7 7', '8 1', '8 2', '9 1', '9 2', '9 3', '9 4', '9 5', '9 6', '10 1', '10 2', '10 3', '10 4', '10 5', '10 6', '11 1', '11 2', '11 3', '11 4', '11 5', '12 1', '12 2', '12 3', '13 1', '13 2', '13 3', '13 4', '14 1', '14 2', '14 3', '14 4', '14 5', '14 6', '15 1', '15 2', '15 3', '15 4', '15 5', '15 6', '15 7', '16 1', '16 2', '16 3', '16 4', '17 1', '17 2', '17 3', '17 4', '17 5', '17 6', '17 7', '18 1', '18 2', '18 3', '18 4', '18 5', '18 6', '18 7', '18 8', '18 9', '19 1', '19 2', '19 3', '19 4', '19 5', '20 1', '20 2', '20 3', '20 4', '20 5', '21 1', '21 2', '21 3', '21 4', '21 5', '22 1', '22 2', '22 3', '22 4', '22 5', '22 6', '23 1', '23 2', '23 3', '23 4', '23 5', '24 1', '24 2', '25 1', '25 2', '25 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feed_names.index(\"17 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Train:\n",
    "    regressor = TransformedTargetRegressor(\n",
    "        regressor = linear_model.LinearRegression(),\n",
    "        transformer = StandardScaler(with_mean = True, with_std = True)\n",
    "        )\n",
    "    #\n",
    "    #(\"preprocess_cs\",StandardScaler(with_mean = True, with_std = True)),\n",
    "    #(\"preprocess_tr\",PowerTransformer(method = \"yeo-johnson\")),\n",
    "    pipeline = Pipeline(\n",
    "        steps = [\n",
    "            (\"preprocess_cs\",StandardScaler(with_mean = True, with_std = True)),\n",
    "            (\"target_transform\", regressor)\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Train:\n",
    "    count = 0\n",
    "    for net in range(1,27):\n",
    "        feed = 0\n",
    "        while True:\n",
    "            feed +=1\n",
    "            start_time = time.time()\n",
    "            #Load datasets\n",
    "            try:\n",
    "                # 10% data\n",
    "                path_10_simulation = base_path.joinpath(\"Simulation_Results_10\")\n",
    "                with open(path_10_simulation.joinpath(f\"Loads_Network_{net}_Feeder_{feed}.npy\"), 'rb') as f:\n",
    "                    Dataset_10 = np.load(f, allow_pickle = True)\n",
    "                with open(path_10_simulation.joinpath(f\"Targets_Network_{net}_Feeder_{feed}.npy\"), \"rb\") as f:\n",
    "                    Targets_10 = np.load(f, allow_pickle = True)\n",
    "                # 30% data\n",
    "                path_30_simulation = base_path.joinpath(\"Simulation_Results_30\")\n",
    "                with open(path_30_simulation.joinpath(f\"Loads_Network_{net}_Feeder_{feed}.npy\"), 'rb') as f:\n",
    "                    Dataset_30 = np.load(f, allow_pickle = True)\n",
    "                with open(path_30_simulation.joinpath(f\"Targets_Network_{net}_Feeder_{feed}.npy\"), \"rb\") as f:\n",
    "                    Targets_30 = np.load(f, allow_pickle = True)\n",
    "                # 50% data\n",
    "                path_50_simulation = base_path.joinpath(\"Simulation_Results_50\")\n",
    "                with open(path_50_simulation.joinpath(f\"Loads_Network_{net}_Feeder_{feed}.npy\"), 'rb') as f:\n",
    "                    Dataset_50 = np.load(f, allow_pickle = True)\n",
    "                with open(path_50_simulation.joinpath(f\"Targets_Network_{net}_Feeder_{feed}.npy\"), \"rb\") as f:\n",
    "                    Targets_50 = np.load(f, allow_pickle = True)\n",
    "                # 70% data\n",
    "                path_70_simulation = base_path.joinpath(\"Simulation_Results_70\")\n",
    "                with open(path_70_simulation.joinpath(f\"Loads_Network_{net}_Feeder_{feed}.npy\"), 'rb') as f:\n",
    "                    Dataset_70 = np.load(f, allow_pickle = True)\n",
    "                with open(path_70_simulation.joinpath(f\"Targets_Network_{net}_Feeder_{feed}.npy\"), \"rb\") as f:\n",
    "                    Targets_70 = np.load(f, allow_pickle = True)\n",
    "                # 100% data\n",
    "                path_100_simulation = base_path.joinpath(\"Simulation_Results_100\")\n",
    "                with open(path_100_simulation.joinpath(f\"Loads_Network_{net}_Feeder_{feed}.npy\"), 'rb') as f:\n",
    "                    Dataset_100 = np.load(f, allow_pickle = True)\n",
    "                with open(path_100_simulation.joinpath(f\"Targets_Network_{net}_Feeder_{feed}.npy\"), \"rb\") as f:\n",
    "                    Targets_100 = np.load(f, allow_pickle = True)\n",
    "            except:\n",
    "                break\n",
    "            print(net, feed)\n",
    "            count += 1\n",
    "            # hyper parameter selection\n",
    "            #linear -> none\n",
    "            # train on 70% and test on 30\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Dataset_10, 230*Targets_10, test_size=0.3, shuffle = False)\n",
    "            #linear model\n",
    "            print(\"linear \", end = \"\")\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            preds_linear_10 = pipeline.predict(X_test)\n",
    "            preds_linear_30 = pipeline.predict(Dataset_30)\n",
    "            preds_linear_50 = pipeline.predict(Dataset_50)\n",
    "            preds_linear_70 = pipeline.predict(Dataset_70)\n",
    "            preds_linear_100 = pipeline.predict(Dataset_100)\n",
    "            results_linear_dict = {\n",
    "                'Observed_10': y_test,\n",
    "                'Predicted_10': preds_linear_10,\n",
    "                'Error_10': preds_linear_10-y_test,\n",
    "                'Observed_30': 230*Targets_30,\n",
    "                'Predicted_30': preds_linear_30,\n",
    "                'Error_30': preds_linear_30-230*Targets_30,\n",
    "                'Observed_50': 230*Targets_50,\n",
    "                'Predicted_50': preds_linear_50,\n",
    "                'Error_50': preds_linear_50-230*Targets_50,\n",
    "                'Observed_70': 230*Targets_70,\n",
    "                'Predicted_70': preds_linear_70,\n",
    "                'Error_70': preds_linear_70-230*Targets_70,\n",
    "                'Observed_100': 230*Targets_100,\n",
    "                'Predicted_100': preds_linear_100,\n",
    "                'Error_100': preds_linear_100-230*Targets_100\n",
    "            }\n",
    "\n",
    "            ## Neural Network\n",
    "\n",
    "            pipeline.set_params(target_transform__regressor = MLPRegressor())\n",
    "\n",
    "            param_grid = {\n",
    "            'target_transform__regressor__learning_rate_init': np.linspace(0.001, 0.1, 20),\n",
    "            'target_transform__regressor__alpha': np.linspace(0.00001, 0.001, 20),\n",
    "            'target_transform__regressor__hidden_layer_sizes': [[25,], [50,],[100,],[150,]]   \n",
    "            }\n",
    "\n",
    "            print(\"NN params \", end = \"\")\n",
    "            grid_search_nn = GridSearchCV(pipeline, param_grid, cv=RepeatedKFold(n_splits=5, n_repeats=2), scoring=rmse_scorer, n_jobs=-1)\n",
    "            grid_search_nn.fit(X_train, y_train)\n",
    "\n",
    "            NN_optimal_params = grid_search_nn.best_params_\n",
    "\n",
    "            print(\"NN train \", end = \"\")\n",
    "            pipeline.set_params(target_transform__regressor__alpha = NN_optimal_params[\"target_transform__regressor__alpha\"])\n",
    "            pipeline.set_params(target_transform__regressor__hidden_layer_sizes = NN_optimal_params[\"target_transform__regressor__hidden_layer_sizes\"])\n",
    "            pipeline.set_params(target_transform__regressor__learning_rate_init = NN_optimal_params[\"target_transform__regressor__learning_rate_init\"])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            preds_nn_10 = pipeline.predict(X_test)\n",
    "            preds_nn_30 = pipeline.predict(Dataset_30)\n",
    "            preds_nn_50 = pipeline.predict(Dataset_50)\n",
    "            preds_nn_70 = pipeline.predict(Dataset_70)\n",
    "            preds_nn_100 = pipeline.predict(Dataset_100)\n",
    "            results_NN_dict = {\n",
    "                'Observed_10': y_test,\n",
    "                'Predicted_10': preds_nn_10,\n",
    "                'Error_10': preds_nn_10-y_test,\n",
    "                'Observed_30': 230*Targets_30,\n",
    "                'Predicted_30': preds_nn_30,\n",
    "                'Error_30': preds_nn_30-230*Targets_30,\n",
    "                'Observed_50': 230*Targets_50,\n",
    "                'Predicted_50': preds_nn_50,\n",
    "                'Error_50': preds_nn_50-230*Targets_50,\n",
    "                'Observed_70': 230*Targets_70,\n",
    "                'Predicted_70': preds_nn_70,\n",
    "                'Error_70': preds_nn_70-230*Targets_70,\n",
    "                'Observed_100': 230*Targets_100,\n",
    "                'Predicted_100': preds_nn_100,\n",
    "                'Error_100': preds_nn_100-230*Targets_100\n",
    "            }\n",
    "\n",
    "\n",
    "            with open(base_path.joinpath(\"Models\").joinpath(\"Results\").joinpath(f\"Network_{net}_Feeder_{feed}_linear_results.pkl\"), 'wb') as file:\n",
    "                pickle.dump(results_linear_dict, file)\n",
    "\n",
    "            with open(base_path.joinpath(\"Models\").joinpath(\"Results\").joinpath(f\"Network_{net}_Feeder_{feed}_NN_results.pkl\"), 'wb') as file:\n",
    "                pickle.dump(results_NN_dict, file)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(\"Took: \", end_time-start_time, \" est. rem (mins): \", (128-count)*(end_time-start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for net in range(1,27):\n",
    "    feed = 0\n",
    "    while True:\n",
    "        feed +=1\n",
    "        #Load datasets\n",
    "        try:\n",
    "            with open(base_path.joinpath(\"Models\").joinpath(\"Results\").joinpath(f\"Network_{net}_Feeder_{feed}_linear_results.pkl\"), 'rb') as file:\n",
    "                loaded_results_linear_dict = pickle.load(file)\n",
    "            data_list.append(loaded_results_linear_dict)\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_nn = []\n",
    "for net in range(1,27):\n",
    "    feed = 0\n",
    "    while True:\n",
    "        feed +=1\n",
    "        #Load datasets\n",
    "        try:\n",
    "            with open(base_path.joinpath(\"Models\").joinpath(\"Results\").joinpath(f\"Network_{net}_Feeder_{feed}_NN_results.pkl\"), 'rb') as file:\n",
    "                loaded_results_nn_dict = pickle.load(file)\n",
    "            data_list_nn.append(loaded_results_nn_dict)\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 4))\n",
    "\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Observed_10\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[0], data = all_errors_df, showfliers=False, linewidth = 1)\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Observed_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[1], data = all_errors_df, showfliers=False, linewidth = 1)\n",
    "\n",
    "\n",
    "axes[0].set_xlabel(\"Network and Feeder Number\", fontsize = 8)\n",
    "axes[1].set_xlabel(\"Network and Feeder Number\", fontsize = 8)\n",
    "\n",
    "axes[0].set_ylabel(\"Voltage\", fontsize = 8)\n",
    "axes[1].set_ylabel(\"Voltage\", fontsize = 8)\n",
    "\n",
    "axes[0].set_ylim(210,260)\n",
    "axes[1].set_ylim(150,350)\n",
    "\n",
    "axes[0].set_title(\"Voltage at 10% PV and EV Penetration\", fontsize = 8)\n",
    "axes[1].set_title(\"Voltage at 100% PV and EV Penetration\", fontsize = 8)\n",
    "\n",
    "axes[0].text(0.95, -0.35, '(a)', ha='center', transform=axes[0].transAxes)\n",
    "axes[1].text(0.95, -0.35, '(b)', ha='center', transform=axes[1].transAxes)\n",
    "\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=-90, fontsize=5)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=-90, fontsize=5) \n",
    "\n",
    "axes[0].set_yticks([210,220,230,240, 250,260], [210,220,230,240, 250,260],fontsize=8)\n",
    "axes[1].set_yticks([150,190,230,270,310,350], [150,190,230,270,310,350], fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Images/voltage_feeder_distribution.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voltges_10 = pd.concat([abs(pd.DataFrame(data[\"Observed_10\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "voltges_100 = pd.concat([abs(pd.DataFrame(data[\"Observed_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "\n",
    "print(sum(voltges_100.values.flatten())/len(voltges_100.values.flatten()))\n",
    "print(np.std(voltges_100.values.flatten()))\n",
    "print(sum(voltges_10.values.flatten())/len(voltges_10.values.flatten()))\n",
    "print(np.std(voltges_10.values.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.concat([abs(pd.DataFrame(data[\"Observed_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "pred_lin_df = pd.concat([abs(pd.DataFrame(data[\"Predicted_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "pred_nn_df = pd.concat([abs(pd.DataFrame(data[\"Predicted_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "\n",
    "Error_NN = {}\n",
    "Error_lin = {}\n",
    "\n",
    "for coloumn in obs_df:\n",
    "    Error_linear = []\n",
    "    Error_nn = []\n",
    "    for i in range(len(obs_df[coloumn])):\n",
    "        if 207 < obs_df[coloumn][i] < 253:\n",
    "            Error_linear.append(abs(obs_df[coloumn][i]-pred_lin_df[coloumn][i]))\n",
    "            Error_nn.append(abs(obs_df[coloumn][i]-pred_nn_df[coloumn][i]))\n",
    "    Error_NN[coloumn] = Error_nn\n",
    "    Error_lin[coloumn] = Error_linear\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(Error_lin.keys())),len(list(Error_lin.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 4))\n",
    "\n",
    "all_errors_df_lin = pd.concat([abs(pd.DataFrame(data[\"Error_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df_lin = all_errors_df_lin.reindex(all_errors_df_lin.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[0], data = all_errors_df_lin, showfliers=False, linewidth = 1)\n",
    "#axes[0].boxplot(list(Error_lin.values()), labels = list(Error_lin.keys()), showfliers=False)\n",
    "\n",
    "\n",
    "all_errors_df_nn = pd.concat([abs(pd.DataFrame(data[\"Error_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df_nn = all_errors_df_nn.reindex(all_errors_df_nn.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[1], data = all_errors_df_nn, showfliers=False, linewidth = 1)\n",
    "#axes[1].boxplot(list(Error_NN.values()), labels = list(Error_NN.keys()), showfliers=False)\n",
    "\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=-90, fontsize=8)  \n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=-90, fontsize=8)  \n",
    "\n",
    "\n",
    "axes[0].set_xlabel(\"Network and Feeder Number\", fontsize = 8)\n",
    "axes[1].set_xlabel(\"Network and Feeder Number\", fontsize = 8)\n",
    "\n",
    "axes[0].set_ylabel(\"Absoloute Error\", fontsize = 8)\n",
    "axes[1].set_ylabel(\"Absoloute Error\", fontsize = 8)\n",
    "\n",
    "axes[0].set_ylim(0,20)\n",
    "#axes[1].set_ylim(0,20)\n",
    "\n",
    "axes[0].text(0.95, -0.35, '(a)', ha='center', transform=axes[0].transAxes)\n",
    "axes[1].text(0.95, -0.35, '(b)', ha='center', transform=axes[1].transAxes)\n",
    "\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=-90, fontsize=5)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=-90, fontsize=5) \n",
    "\n",
    "ytick = range(0,25,5)\n",
    "ytick_1 = range(0,450,50)\n",
    "\n",
    "axes[0].set_yticks(ytick, ytick,fontsize=8)\n",
    "axes[1].set_yticks(ytick_1, ytick_1, fontsize=8)\n",
    "\n",
    "axes[0].set_title(\"Linear Regression Error\",fontsize=8)\n",
    "axes[1].set_title(\"Neural Network Regression Error\",fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Images/voltage_error_feeder_distribution.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(3.5, 2.2))\n",
    "abcd = pd.DataFrame(data = [list(all_errors_df_lin.mean()), list(all_errors_df_nn.mean())], index = [\"Linear\", \"Neural Network\"]).T\n",
    "sns.histplot(ax = axes[0], data = abcd[\"Linear\"])\n",
    "sns.histplot(ax = axes[1], data = abcd[\"Neural Network\"])\n",
    "axes[0].set_ylabel(\"Count\", fontsize = 8)\n",
    "axes[0].set_xlabel(\"Mean Absolute Error (V)\", fontsize = 8)\n",
    "axes[1].set_ylabel(\"Count\", fontsize = 8)\n",
    "axes[1].set_xlabel(\"Mean Absolute Error (V)\", fontsize = 8)\n",
    "\n",
    "axes[0].set_yticks(range(0,60,10), range(0,60,10),fontsize=8)\n",
    "axes[1].set_yticks(range(0,90,20), range(0,90,20),fontsize=8)\n",
    "axes[0].set_xticks(range(0,7,2),range(0,7,2),fontsize=8)\n",
    "axes[1].set_xticks(range(0,90,20), range(0,90,20),fontsize=8)\n",
    "\n",
    "axes[0].text(0.55, -0.34, '(a)', va='center', ha='right', transform=axes[0].transAxes, fontsize=8)\n",
    "axes[1].text(0.55, -0.34, '(b)', va='center', ha='right', transform=axes[1].transAxes, fontsize=8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (10,4))\n",
    "sns.lineplot(voltges_10[\"1 1\"].values[:288], label = \"10%\")\n",
    "sns.lineplot(voltges_100[\"1 1\"].values[:288], label = \"100%\")\n",
    "\n",
    "plt.title(\"Voltage For Network Feeder 1\")\n",
    "plt.xlabel(\"Sample [5 mins]\")\n",
    "plt.ylabel(\"Voltage\")\n",
    "plt.xlim(0, 288)\n",
    "plt.ylim(205, 255)\n",
    "plt.axhline(230*1.1, linestyle = \"--\", color = \"red\", label = \"Voltage Limits\")\n",
    "plt.axhline(230*0.9, linestyle = \"--\", color = \"red\")\n",
    "plt.legend(title = \"PV and EV Pentration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Overall stats\n",
    "for t in [\"10\", \"30\", \"50\", \"70\", \"100\"]:\n",
    "    error_NN = pd.concat([abs(pd.DataFrame(data[f\"Error_{t}\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "    error_lin = pd.concat([abs(pd.DataFrame(data[f\"Error_{t}\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list)], axis=1).drop(\"13 4\", axis = 1)\n",
    "\n",
    "    flattened_NN = error_NN.values.flatten()\n",
    "    flattened_lin = error_lin.values.flatten()\n",
    "\n",
    "    # Calculate the average of all values in the list\n",
    "\n",
    "    print(t)\n",
    "    print(\"Average NN:\", sum(flattened_NN) / len(flattened_NN))\n",
    "    print(\"max NN\", max(error_NN.mean()))\n",
    "    #print(\"max lin\", max(error_lin.mean()))\n",
    "    #print(\"Average lin:\", sum(flattened_lin) / len(flattened_lin))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,2, figsize = (6.3, 8))\n",
    "\n",
    "for i in range(5): \n",
    "    ax[i][0].axvline(230*1.1, color=\"red\", linestyle=\"dashed\", linewidth=1, alpha=1)\n",
    "    ax[i][0].axvline(230*0.9, color=\"red\", linestyle=\"dashed\", linewidth=1, alpha=1)\n",
    "    ax[i][0].set_xlim(180, 280)\n",
    "    ax[i][0].set_ylim(180, 280)\n",
    "    ax[i][0].set_xlabel(\"Observed Voltage\", fontsize = 8)\n",
    "    ax[i][0].set_ylabel(\"Predicted Voltage\", fontsize = 8)\n",
    "    \n",
    "    ax[i][1].axvline(230*1.1, color=\"red\", linestyle=\"dashed\", linewidth=1, alpha=1)\n",
    "    ax[i][1].axvline(230*0.9, color=\"red\", linestyle=\"dashed\", linewidth=1, alpha=1)\n",
    "    ax[i][1].set_xlim(180, 280)\n",
    "    ax[i][1].set_ylim(180, 280)\n",
    "    ax[i][1].set_xlabel(\"Observed Voltage\", fontsize = 8)\n",
    "    ax[i][1].set_ylabel(\"Predicted Voltage\", fontsize = 8)\n",
    "    \n",
    "    #ax[i][0].legend(loc = 'upper left', fontsize = 8)\n",
    "    #ax[i][1].legend(loc = 'upper left', fontsize = 8)\n",
    "    \n",
    "    sns.lineplot(ax = ax[i][0], x = [0,500], y = [0,500], color = \"black\", linewidth=1, linestyle = \"dashed\", alpha = 0.5)\n",
    "    sns.lineplot(ax = ax[i][1], x = [0,500], y = [0,500], color = \"black\", linewidth=1, linestyle = \"dashed\", alpha = 0.5)\n",
    "    \n",
    "    fig_range = [180,200,220,240,260,280]\n",
    "    ax[i][0].set_xticks(fig_range, fig_range, fontsize=8)\n",
    "    ax[i][0].set_yticks(fig_range, fig_range, fontsize=8)\n",
    "    ax[i][1].set_xticks(fig_range, fig_range, fontsize=8)\n",
    "    ax[i][1].set_yticks(fig_range, fig_range, fontsize=8)\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(128):\n",
    "    if i == 63:\n",
    "        continue\n",
    "    sns.scatterplot(ax = ax[0][0], x = data_list[i][\"Observed_10\"],y = data_list[i][\"Predicted_10\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    sns.scatterplot(ax = ax[0][1], x = data_list_nn[i][\"Observed_10\"],y = data_list_nn[i][\"Predicted_10\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    \n",
    "    sns.scatterplot(ax = ax[1][0], x = data_list[i][\"Observed_30\"],y = data_list[i][\"Predicted_30\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    sns.scatterplot(ax = ax[1][1], x = data_list_nn[i][\"Observed_30\"],y = data_list_nn[i][\"Predicted_30\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    \n",
    "    sns.scatterplot(ax = ax[2][0], x = data_list[i][\"Observed_50\"],y = data_list[i][\"Predicted_50\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    sns.scatterplot(ax = ax[2][1], x = data_list_nn[i][\"Observed_50\"],y = data_list_nn[i][\"Predicted_50\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    \n",
    "    sns.scatterplot(ax = ax[3][0], x = data_list[i][\"Observed_70\"],y = data_list[i][\"Predicted_70\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    sns.scatterplot(ax = ax[3][1], x = data_list_nn[i][\"Observed_70\"],y = data_list_nn[i][\"Predicted_70\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    \n",
    "    sns.scatterplot(ax = ax[4][0], x = data_list[i][\"Observed_100\"],y = data_list[i][\"Predicted_100\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    sns.scatterplot(ax = ax[4][1], x = data_list_nn[i][\"Observed_100\"],y = data_list_nn[i][\"Predicted_100\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    \n",
    "\n",
    "ax[0][0].set_title(\"Linear Regression\", fontsize = 12)\n",
    "ax[0][1].set_title(\"Neural Network\", fontsize = 12)\n",
    "\n",
    "ax[0][0].text(0.95, -0.3, '(a)', va='center', ha='right', transform=ax[0][0].transAxes, fontsize=8)\n",
    "ax[0][1].text(0.95, -0.3, '(b)', va='center', ha='right', transform=ax[0][1].transAxes, fontsize=8)\n",
    "\n",
    "ax[1][0].text(0.95, -0.3, '(c)', va='center', ha='right', transform=ax[1][0].transAxes, fontsize=8)\n",
    "ax[1][1].text(0.95, -0.3, '(d)', va='center', ha='right', transform=ax[1][1].transAxes, fontsize=8)\n",
    "\n",
    "ax[2][0].text(0.95, -0.3, '(e)', va='center', ha='right', transform=ax[2][0].transAxes, fontsize=8)\n",
    "ax[2][1].text(0.95, -0.3, '(f)', va='center', ha='right', transform=ax[2][1].transAxes, fontsize=8)\n",
    "\n",
    "ax[3][0].text(0.95, -0.3, '(g)', va='center', ha='right', transform=ax[3][0].transAxes, fontsize=8)\n",
    "ax[3][1].text(0.95, -0.3, '(h)', va='center', ha='right', transform=ax[3][1].transAxes, fontsize=8)\n",
    "\n",
    "ax[4][0].text(0.95, -0.3, '(i)', va='center', ha='right', transform=ax[4][0].transAxes, fontsize=8)\n",
    "ax[4][1].text(0.95, -0.3, '(j)', va='center', ha='right', transform=ax[4][1].transAxes, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Images/performance.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize = (10, 10))\n",
    "\n",
    "for i in range(1): \n",
    "    ax[0].set_xlim(220, 250)\n",
    "    ax[0].set_ylim(220, 250)\n",
    "    ax[0].set_xlabel(\"Observed Voltage\", fontsize = 12)\n",
    "    ax[0].set_ylabel(\"Predicted Voltage\", fontsize = 12)\n",
    "    \n",
    "    ax[1].set_xlim(220, 250)\n",
    "    ax[1].set_ylim(220, 250)\n",
    "    ax[1].set_xlabel(\"Observed Voltage\", fontsize = 12)\n",
    "    ax[1].set_ylabel(\"Predicted Voltage\", fontsize = 12)\n",
    "    \n",
    "    #ax[i][0].legend(loc = 'upper left', fontsize = 8)\n",
    "    #ax[i][1].legend(loc = 'upper left', fontsize = 8)\n",
    "    \n",
    "    sns.lineplot(ax = ax[0], x = [0,500], y = [0,500], color = \"black\", linewidth=1, linestyle = \"dashed\", alpha = 0.5)\n",
    "    sns.lineplot(ax = ax[1], x = [0,500], y = [0,500], color = \"black\", linewidth=1, linestyle = \"dashed\", alpha = 0.5)\n",
    "    \n",
    "    fig_range = [220,240]\n",
    "    #ax[0].set_xticks(fig_range, fig_range, fontsize=8)\n",
    "    #ax[1].set_xticks(fig_range, fig_range, fontsize=8)\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(128):\n",
    "    if i == 63:\n",
    "        continue\n",
    "    sns.scatterplot(ax = ax[0], x = data_list[i][\"Observed_10\"],y = data_list[i][\"Predicted_10\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    sns.scatterplot(ax = ax[1], x = data_list_nn[i][\"Observed_10\"],y = data_list_nn[i][\"Predicted_10\"], alpha = 0.1, color = \"black\", s = 1)\n",
    "    \n",
    "\n",
    "ax[0].set_title(\"Linear Regression\", fontsize = 15)\n",
    "ax[1].set_title(\"Neural Network\", fontsize = 15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(20, 15))\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Error_10\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[0], data = all_errors_df)\n",
    "\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Error_30\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[1], data = all_errors_df)\n",
    "\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Error_50\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[2], data = all_errors_df)\n",
    "\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Error_70\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[3], data = all_errors_df)\n",
    "\n",
    "\n",
    "all_errors_df = pd.concat([abs(pd.DataFrame(data[\"Error_100\"], columns=[Feed_names[i]])) for i, data in enumerate(data_list_nn)], axis=1).drop(\"13 4\", axis = 1)\n",
    "all_errors_df = all_errors_df.reindex(all_errors_df.median().sort_values().index, axis=1)\n",
    "sns.boxplot(ax = axes[4], data = all_errors_df)\n",
    "\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=-90, fontsize=8)  \n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=-90, fontsize=8)  \n",
    "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=-90, fontsize=8)  \n",
    "axes[3].set_xticklabels(axes[3].get_xticklabels(), rotation=-90, fontsize=8)  \n",
    "axes[4].set_xticklabels(axes[4].get_xticklabels(), rotation=-90, fontsize=8)   \n",
    "\n",
    "axes[0].set_title(\"Voltage Error at 10% PV and EV Penetration\")\n",
    "axes[1].set_title(\"Voltage Error at 30% PV and EV Penetration\")\n",
    "axes[2].set_title(\"Voltage Error at 50% PV and EV Penetration\")\n",
    "axes[3].set_title(\"Voltage Error at 70% PV and EV Penetration\")\n",
    "axes[4].set_title(\"Voltage Error at 100% PV and EV Penetration\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "for i in range(128):\n",
    "    if i == 63:\n",
    "        continue\n",
    "    sns.scatterplot(x = data_list_nn[i][\"Observed_100\"],y = data_list_nn[i][\"Predicted_100\"], alpha = 0.01, color = \"black\")\n",
    "sns.lineplot(x = [0,500], y = [0,500], color = \"black\", linestyle = \"dashed\", alpha = 0.5)\n",
    "plt.plot([253, 253], [100, 400], color=\"red\", linestyle=\"dashed\", linewidth=1, alpha=1, label = \"Voltage Limits\")\n",
    "plt.plot([207, 207], [100, 400], color=\"red\", linestyle=\"dashed\", linewidth=1, alpha=1)\n",
    "plt.xlim(180, 280)\n",
    "plt.ylim(180, 280)\n",
    "plt.title(\"Neural Network Observed vs Predicted Voltage\")\n",
    "plt.xlabel(\"Observed Voltage\")\n",
    "\n",
    "plt.ylabel(\"Predicted Voltage\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,8))\n",
    "for i in range(128):\n",
    "    if i == 63:\n",
    "        continue\n",
    "    sns.scatterplot(x = data_list_nn[i][\"Observed_100\"],y = data_list_nn[i][\"Error_100\"], alpha = 0.03)\n",
    "#sns.lineplot(x = [0,500], y = [0,500], color = \"black\", linestyle = \"dashed\", alpha = 0.5)\n",
    "#plt.xlim(140, 350)\n",
    "#plt.ylim(140, 350)\n",
    "plt.title(\"Neural Network\")\n",
    "plt.xlabel(\"Observed Voltage\")\n",
    "plt.ylabel(\"Voltage Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count overvoltage\n",
    "#gen 2 dataframes, Percent by violations\n",
    "\n",
    "obs_viol_df = pd.DataFrame()\n",
    "for name in [\"Observed_10\",\"Observed_30\",\"Observed_50\",\"Observed_70\", \"Observed_100\",\"Predicted_10\",\"Predicted_30\",\"Predicted_50\",\"Predicted_70\", \"Predicted_100\"]:\n",
    "    Upper = []\n",
    "    Lower = []\n",
    "    for i in range(128):\n",
    "        if i == 63:\n",
    "            continue\n",
    "        upper_violation_count = 0\n",
    "        lower_violation_count = 0\n",
    "        for data in data_list_nn[i][name]:\n",
    "            if data > 230 + 23:\n",
    "                upper_violation_count += 1\n",
    "            if data < 230-23:\n",
    "                upper_violation_count += 1\n",
    "        Upper.append(upper_violation_count)\n",
    "        Lower.append(lower_violation_count)\n",
    "    obs_viol_df[name + \" Upper\"] = Upper\n",
    "    obs_viol_df[name + \" Lower\"] = Lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[0,6,12,26,35],[0,16,22,31,41],[1,6,10,16,22],[1,11,17,25,33]], columns = [\"10%\",\"30%\", \"50%\",\"70%\",\"100%\"])\n",
    "custom_index = [\"Observed Upper\",\"Predicted Upper\",\"Observed Lower\", \"Predicted Lower\"]\n",
    "df.index = custom_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_viol_df = obs_viol_df.rename(index=dict(zip(range(0,128), Feed_names[:62] + Feed_names[63:])))\n",
    "obs_viol_df = obs_viol_df.sort_values(by='Predicted_100 Upper', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_viol_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize = (6.3,4))\n",
    "ax[0].set_title(\"Predicted Number of Violations\", fontsize = 8)\n",
    "c = sns.color_palette(\"hls\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Predicted_100 Upper\", x = obs_viol_df.index, color = c[0], alpha = 0.75, label = \"100%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Predicted_70 Upper\", x = obs_viol_df.index, color =  c[1], alpha = 0.75, label = \"70%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Predicted_50 Upper\", x = obs_viol_df.index, color =  c[2], alpha = 0.75, label = \"50%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Predicted_30 Upper\", x = obs_viol_df.index, color =  c[3], alpha = 0.75, label = \"30%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Predicted_10 Upper\", x = obs_viol_df.index, color =  c[4], alpha = 0.75, label = \"10%\")\n",
    "\n",
    "\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Predicted_100 Lower\", x = obs_viol_df.index, color = c[0], alpha = 0.75, label = \"100%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Predicted_70 Lower\", x = obs_viol_df.index, color =  c[1], alpha = 0.75, label = \"70%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Predicted_50 Lower\", x = obs_viol_df.index, color =  c[2], alpha = 0.75, label = \"50%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Predicted_30 Lower\", x = obs_viol_df.index, color =  c[3], alpha = 0.75, label = \"30%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Predicted_10 Lower\", x = obs_viol_df.index, color =  c[4], alpha = 0.75, label = \"10%\")\n",
    "\n",
    "\n",
    "ax[0].set_ylabel(\"Upper Limit Violation Count\", fontsize=8)\n",
    "ax[1].set_ylabel(\"Lower Limit Violation Count\", fontsize=8)\n",
    "ax[0].set_xlabel(\"Network and Feeder Number\", fontsize=8)\n",
    "ax[1].set_xlabel(\"Network and Feeder Number\", fontsize=8)\n",
    "\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-90, fontsize=4); \n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=-90, fontsize=4);\n",
    "ax[0].set_yticklabels(ax[0].get_yticklabels(), fontsize=8); \n",
    "ax[1].set_yticklabels(ax[1].get_yticklabels(), fontsize=8);\n",
    "\n",
    "leg1 = ax[0].legend(title = \"EV and PV Penetration\", fontsize=8, loc = \"upper right\")\n",
    "leg2 = ax[1].legend(title = \"EV and PV Penetration\", fontsize=8, loc = \"upper right\")\n",
    "leg1.get_title().set_fontsize('8')\n",
    "leg2.get_title().set_fontsize('8')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_viol_df = obs_viol_df.sort_values(by='Observed_100 Upper', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize = (6.3,4))\n",
    "ax[0].set_title(\"Observed Number of Violations\")\n",
    "c = sns.color_palette(\"hls\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Observed_100 Upper\", x = obs_viol_df.index, color = c[0], alpha = 0.75, label = \"100%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Observed_70 Upper\", x = obs_viol_df.index, color =  c[1], alpha = 0.75, label = \"70%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Observed_50 Upper\", x = obs_viol_df.index, color =  c[2], alpha = 0.75, label = \"50%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Observed_30 Upper\", x = obs_viol_df.index, color =  c[3], alpha = 0.75, label = \"30%\")\n",
    "sns.barplot(ax = ax[0], data = obs_viol_df, y = \"Observed_10 Upper\", x = obs_viol_df.index, color =  c[4], alpha = 0.75, label = \"10%\")\n",
    "\n",
    "\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Observed_100 Lower\", x = obs_viol_df.index, color = c[0], alpha = 0.75, label = \"100%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Observed_70 Lower\", x = obs_viol_df.index, color =  c[1], alpha = 0.75, label = \"70%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Observed_50 Lower\", x = obs_viol_df.index, color =  c[2], alpha = 0.75, label = \"50%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Observed_30 Lower\", x = obs_viol_df.index, color =  c[3], alpha = 0.75, label = \"30%\")\n",
    "sns.barplot(ax = ax[1], data = obs_viol_df, y = \"Observed_10 Lower\", x = obs_viol_df.index, color =  c[4], alpha = 0.75, label = \"10%\")\n",
    "\n",
    "\n",
    "ax[0].set_ylabel(\"Upper Limit Violation Count\", fontsize=8)\n",
    "ax[1].set_ylabel(\"Lower Limit Violation Count\", fontsize=8)\n",
    "ax[0].set_xlabel(\"Network and Feeder Number\", fontsize=8)\n",
    "ax[1].set_xlabel(\"Network and Feeder Number\", fontsize=8)\n",
    "\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-90, fontsize=4); \n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=-90, fontsize=4);\n",
    "ax[0].set_yticklabels(ax[0].get_yticklabels(), fontsize=8); \n",
    "ax[1].set_yticklabels(ax[1].get_yticklabels(), fontsize=8);\n",
    "\n",
    "leg1 = ax[0].legend(title = \"EV and PV Penetration\", fontsize=8)\n",
    "leg2 = ax[1].legend(title = \"EV and PV Penetration\", fontsize=8)\n",
    "leg1.get_title().set_fontsize('8')\n",
    "leg2.get_title().set_fontsize('8')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot sugests that networks with high numbers of upper violations have a high liklyhood of crossing the lower limit in direct contradiction to the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (6.3,2.5))\n",
    "ax.set_title(\"Predicted Number of Violations\", fontsize = 8)\n",
    "c = sns.color_palette(\"hls\")\n",
    "sns.barplot(ax = ax, data = obs_viol_df, y = \"Predicted_100 Upper\", x = obs_viol_df.index, color = c[0], alpha = 0.75, label = \"100%\")\n",
    "sns.barplot(ax = ax, data = obs_viol_df, y = \"Predicted_70 Upper\", x = obs_viol_df.index, color =  c[1], alpha = 0.75, label = \"70%\")\n",
    "sns.barplot(ax = ax, data = obs_viol_df, y = \"Predicted_50 Upper\", x = obs_viol_df.index, color =  c[2], alpha = 0.75, label = \"50%\")\n",
    "sns.barplot(ax = ax, data = obs_viol_df, y = \"Predicted_30 Upper\", x = obs_viol_df.index, color =  c[3], alpha = 0.75, label = \"30%\")\n",
    "sns.barplot(ax = ax, data = obs_viol_df, y = \"Predicted_10 Upper\", x = obs_viol_df.index, color =  c[4], alpha = 0.75, label = \"10%\")\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Violation Count\", fontsize=8)\n",
    "ax.set_xlabel(\"Network and Feeder Number\", fontsize=8)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=-90, fontsize=4); \n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=8); \n",
    "\n",
    "leg1 = ax.legend(title = \"EV and PV Penetration\", fontsize=8, loc = \"upper right\")\n",
    "leg1.get_title().set_fontsize('8')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTFXHorQoKLi7KtPpdtKEq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
